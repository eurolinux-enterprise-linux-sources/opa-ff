#!/bin/bash

# PURPOSE:
#
# This file sets environment variables for a SHMEM job started using openmpi
# Note that there are many, many such variables.
# See http://www.open-mpi.org/faq/?category=openfabrics for details, or use
# the command:
#
# $ ompi_info
#
# to get a list of all current settings.

# SYNTAX:
#
# This file must be a valid BASH script. In general, anything that's valid
# in BASH is valid here. To pass variables to openmpi, however, they
# should take this form:
#
# export OMPI_VARIABLE_NAME=variablevalue
#

# SAMPLE Tuning variables:
#
# Uncomment the following lines to enable them.
#
. /usr/sbin/opagetvf_env	# defines bash function opagetvf_func
export MPI_CMD_ARGS=

# These 4 lines select a Virtual Fabric by name and configure PKEY, SL, MTU
# opagetvf2_func "-d 'Compute'" OMPI_MCA_btl_openib_pkey OMPI_MCA_btl_openib_ib_service_level OMPI_MCA_btl_openib_mtu
#export OMPI_MCA_mtl_psm_ib_pkey=$OMPI_MCA_btl_openib_pkey
#export OMPI_MCA_mtl_psm_ib_service_level=$OMPI_MCA_btl_openib_ib_service_level
#export PSM_MTU=$OMPI_MCA_btl_openib_mtu

# These 4 lines select a Virtual Fabric by ServiceId and configure PKEY, SL, MTU
# opagetvf2_func "-S '0x1000117500000000'" OMPI_MCA_btl_openib_pkey OMPI_MCA_btl_openib_ib_service_level OMPI_MCA_btl_openib_mtu
#export OMPI_MCA_mtl_psm_ib_pkey=$OMPI_MCA_btl_openib_pkey
#export OMPI_MCA_mtl_psm_ib_service_level=$OMPI_MCA_btl_openib_ib_service_level
#export PSM_MTU=$OMPI_MCA_btl_openib_mtu

# These 2 lines select a Virtual Fabric by ServiceId and uses dist_sa
# to directly fetch the PathRecord at job startup.  This approach is
# required when using Mesh/Torus fabrics and optional for other topologies
# This mechanism is only supported for Intel HCAs when using PSM (-qlc MPIs)
#export OMPI_MCA_mtl_psm_path_query=opp
#export OMPI_MCA_mtl_psm_ib_service_id=0x1000117500000000

# This line can enable dispersive routing. The following choices are allowed:
#    adaptive, static_src, static_dest, static_base
# If LMC is enabled in the SM, adaptive will automatically be used.
# This mechanism is only supported for Intel HCAs when using PSM (-qlc MPIs)
#export MPI_CMD_ARGS="$MPI_CMD_ARGS PSM_PATH_SELECTION=adaptive"

# Use this to specify a pkey (for virtual fabrics)
#export OMPI_MCA_btl_openib_pkey=0x9002

# Use this to specify a service level (for virtual fabrics)
#export OMPI_MCA_btl_openib_ib_service_level=0

# Use this to specify a mtu (for virtual fabrics)
# 1=256, 2=512, 3=1024, 4=2048, 5=4096
#export OMPI_MCA_btl_openib_mtu=4
#export PSM_MTU=4

# By default, if openmpi cannot find a working IB connection to a target
# node, it will automatically fall back to using Ethernet. If this is not
# what you want, you can use this setting to force openmpi to fail if
# it cannot connect over IB.
#export OMPI_MCA_btl=openib,self

# Use this to cause openmpi to dump the state of all its configuration
# variables
#export OMPI_MCA_mpi_show_mca_params=1

# Default settings to use for a topology where congestion management may improve
# performance. Allows no data paths to inundate the fabric to a point where other
# data paths are starved of bandwidth due to head of queue blocking. Note that
# Congestion Control settings must be made on switches via SM configuration file.
#export MPI_CMD_ARGS="$MPI_CMD_ARGS PSM_DISABLE_CCA=0 PSM_CCTI_INCREMENT=1 PSM_CCTI_TIMER=1 PSM_CCTI_TABLE_SIZE=128"

# These values can enable and control PSM Multi-Rail
# The sample shown is for Dual HCA with port 1 per HCA connected
#export MPI_CMD_ARGS="$MPI_CMD_ARGS PSM_MULTIRAIL=1 PSM_MULTIRAIL_MAP=0:1,1:1"

# This can be enabled to force benchmarks to run on selected CPU cores
#export MPI_TASKSET="${MPI_TASKSET:- -c 1-7}"
