#!/bin/bash

# PURPOSE:
#
# This file sets environment variables for a SHMEM job started using mvapich2
# Note that there are many, many such variables.
# See http://mvapich.cse.ohio-state.edu/support/ for details.

# SYNTAX:
#
# This file must be a valid BASH script. In general, anything that's valid
# in BASH is valid here. To pass variables to mvapich2, however, they
# should take this form:
#
# export MPI_CMD_ARGS="$MPI_CMD_ARGS MV2_VARIABLE_NAME=variablevalue"
#

# SAMPLE Tuning variables:
#
# Uncomment the following lines to enable them.
#
. /usr/sbin/opagetvf_env	# defines bash function opagetvf_func
export MPI_CMD_ARGS=

# These 2 lines select a Virtual Fabric by name and configure PKEY, SL, MTU
# opagetvf_func "-d 'Compute'" MV2_DEFAULT_PKEY MV2_DEFAULT_SERVICE_LEVEL MTU
# export MPI_CMD_ARGS="MV2_DEFAULT_PKEY=$MV2_DEFAULT_PKEY MV2_DEFAULT_SERVICE_LEVEL=$MV2_DEFAULT_SERVICE_LEVEL MV2_DEFAULT_MTU=IBV_MTU_$MTU PSM_PKEY=$MV2_DEFAULT_PKEY IPATH_SL=$MV2_DEFAULT_SERVICE_LEVEL PSM_MTU=$MTU"

# These 2 lines select a Virtual Fabric by ServiceId and configure PKEY, SL, MTU
# opagetvf_func "-S '0x1000117500000000'" MV2_DEFAULT_PKEY MV2_DEFAULT_SERVICE_LEVEL MTU
# export MPI_CMD_ARGS="MV2_DEFAULT_PKEY=$MV2_DEFAULT_PKEY MV2_DEFAULT_SERVICE_LEVEL=$MV2_DEFAULT_SERVICE_LEVEL MV2_DEFAULT_MTU=IBV_MTU_$MTU PSM_PKEY=$MV2_DEFAULT_PKEY IPATH_SL=$MV2_DEFAULT_SERVICE_LEVEL PSM_MTU=$MTU"

# These line selects a Virtual Fabric by ServiceId and uses dist_sa
# to directly fetch the PathRecord at job startup.  This approach is
# required when using Mesh/Torus fabrics and optional for other topologies.
# This mechanism is only supported for Intel HCAs when using PSM (-qlc MPIs)
#export MPI_CMD_ARGS="PSM_PATH_REC=opp PSM_IB_SERVICE_ID=0x1000117500000000"

# This line can enable dispersive routing. The following choices are allowed:
#    adaptive, static_src, static_dest, static_base
# If LMC is enabled in the SM, adaptive will automatically be used.
# This mechanism is only supported for Intel HCAs when using PSM (-qlc MPIs)
#export MPI_CMD_ARGS="$MPI_CMD_ARGS PSM_PATH_SELECTION=adaptive"

# Use this to specify a pkey (for virtual fabrics)
#export MPI_CMD_ARGS="$MPI_CMD_ARGS MV2_DEFAULT_PKEY=0x9002"

# Use this to specify a service level (for virtual fabrics)
#export MPI_CMD_ARGS="$MPI_CMD_ARGS MV2_DEFAULT_SERVICE_LEVEL=0"

# Use this to specify a MTU (for virtual fabrics)
#export MPI_CMD_ARGS="$MPI_CMD_ARGS MV2_DEFAULT_MTU=IBV_MTU_2048 PSM_MTU=2048"

# Note that to use the rdma_cm, support for it must be enabled when mvapich2 
# is compiled.
#export MPI_CMD_ARGS="$MPI_CMD_ARGS MV2_USE_RDMA_CM=1"

# Default settings to use for a topology where congestion management may improve
# performance. Allows no data paths to inundate the fabric to a point where other
# data paths are starved of bandwidth due to head of queue blocking. Note that
# Congestion Control settings must be made on switches via SM configuration file.
#export MPI_CMD_ARGS="$MPI_CMD_ARGS PSM_DISABLE_CCA=0 PSM_CCTI_INCREMENT=1 PSM_CCTI_TIMER=1 PSM_CCTI_TABLE_SIZE=128"

# These values can enable and control PSM Multi-Rail
# The sample shown is for Dual HCA with port 1 per HCA connected
#export MPI_CMD_ARGS="$MPI_CMD_ARGS PSM_MULTIRAIL=1 PSM_MULTIRAIL_MAP=0:1,1:1"

# This can be enabled to force benchmarks to run on selected CPU cores
#export MPI_TASKSET="${MPI_TASKSET:- -c 1-7}"
